{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "whdwwhDWlP7l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "headers = {\n",
        "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
        "    'Accept-Language': 'en-US,en;q=0.8',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    'Cache-Control': 'max-age=0',\n",
        "    'Connection': 'keep-alive',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxiw6RNPV7S9"
      },
      "source": [
        "### 1. Scraping Article Links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpFaFe0DkuR_",
        "outputId": "92b87614-52e2-4585-9a6d-9ed44f28b573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 646/646 [26:50<00:00,  2.49s/it]\n",
            "100%|██████████| 689/689 [48:16<00:00,  4.20s/it]\n",
            "100%|██████████| 696/696 [53:49<00:00,  4.64s/it]\n",
            "100%|██████████| 291/291 [21:12<00:00,  4.37s/it]\n",
            "100%|██████████| 156/156 [09:31<00:00,  3.66s/it]\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "not_working_links = []\n",
        "pref = 'https://www.geeksforgeeks.org/'\n",
        "\n",
        "categories = ['basic','easy','medium','hard','expert']\n",
        "\n",
        "for cat in categories:                                        # Going through each Category\n",
        "\n",
        "  link = pref + cat\n",
        "  r = requests.get(link, headers=headers, allow_redirects=False)\n",
        "  soup = BeautifulSoup(r.text, 'html.parser')\n",
        "  pages = int(soup.find_all('a', class_ = 'page')[-2].text)\n",
        "\n",
        "  for page in tqdm(range(1,pages + 1)):                             # Going through each page\n",
        "\n",
        "    link_ = link + '/' +  str(page)\n",
        "\n",
        "    try:\n",
        "      r = requests.get(link_, headers=headers, allow_redirects=False)\n",
        "      sp = BeautifulSoup(r.text, 'html.parser')\n",
        "      articles = sp.find('div', class_ = 'articles-list').find_all('div', class_ = 'content')\n",
        "      for article in articles:\n",
        "\n",
        "        title_ar = article.find('div', class_ = 'head').text.strip()\n",
        "        link_ar  = article.find('div', class_ = 'head').find('a').get('href')\n",
        "        tags     = ','.join([ ar.find('a').text.strip() for ar in article.find('div', class_ = 'tags-list').find_all('div',class_ = 'tags-list_item')])\n",
        "        data.append([title_ar,link_ar,tags])\n",
        "    except:\n",
        "      not_working_links.append(link_)\n",
        "\n",
        "  pd.DataFrame(data, columns = ['title','link','tags']).to_csv('data.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLxNDShV-R-"
      },
      "source": [
        "### 2. Defining Function to Scrape Article Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C38En8PsWA0q"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "def download(st,nd,df):\n",
        "  data = []\n",
        "\n",
        "  for link in tqdm(df['link'][st:nd]):\n",
        "\n",
        "    r = requests.get(link, headers=headers, allow_redirects=False)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    try:\n",
        "      id_ = [_ for _ in soup.find('body').get('class') if 'postid' in _][0].split('-')[-1]\n",
        "    except:\n",
        "      id_ = np.nan\n",
        "    try:\n",
        "      title = soup.find('h1').text.strip()\n",
        "    except:\n",
        "      title = np.nan\n",
        "    try:\n",
        "      text = soup.find('article').find('div', class_ = 'text').text.strip()\n",
        "    except:\n",
        "      text = np.nan\n",
        "    try:\n",
        "      author_name = soup.find('div', class_ = 'name').text.strip()\n",
        "      author_id   = soup.find('div', class_ = 'name').find('a').get('href').split('/')[4].strip()\n",
        "    except:\n",
        "      author_name = np.nan\n",
        "      author_id   = np.nan\n",
        "    try:\n",
        "      tags = ','.join([_.text for _ in soup.find('div', class_ = 'improved').find_all('li')])\n",
        "    except:\n",
        "      tags = np.nan\n",
        "    try:\n",
        "      img_links   = ','.join([_.get('src') for _ in soup.find('article').find('div', class_ = 'text').find_all('img')])\n",
        "      no_of_img   = len([_.get('src') for _ in soup.find('article').find('div', class_ = 'text').find_all('img')])\n",
        "    except:\n",
        "      img_links = np.nan\n",
        "      no_of_img = np.nan\n",
        "    try:\n",
        "      file_path = 'articles/' + id_ + '.txt'\n",
        "      fd = open(file_path, 'w')\n",
        "      fd.write(text)\n",
        "      fd.close()\n",
        "    except:\n",
        "      file_path = np.nan\n",
        "\n",
        "    data.append([id_, title, author_name, author_id, tags, no_of_img,file_path,link,img_links])\n",
        "\n",
        "  df_ = pd.DataFrame(data, columns = ['id','title','author_name','author_id',\n",
        "                                     'tags','no_of_imgs','file_path','link','img_links'])\n",
        "  df_.to_csv(str(st) + '_' + str(nd) + '.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Scraping Articles Data with MultiProcessing"
      ],
      "metadata": {
        "id": "3ZjkzGxnDh30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP72TXZXWJ-G",
        "outputId": "d6da0d67-36a9-4202-a7a6-6034ffc69179"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 659/8000 [16:01<3:36:33,  1.77s/it]<ipython-input-6-a5ace1c1617a>:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(r.text, 'html.parser')\n",
            " 32%|███▏      | 2587/8000 [1:07:32<3:26:00,  2.28s/it]"
          ]
        }
      ],
      "source": [
        "p1 = multiprocessing.Process(target = download, args = (1 , 5000,df))\n",
        "p2 = multiprocessing.Process(target = download, args = (5000 , 10000,df))\n",
        "p3 = multiprocessing.Process(target = download, args = (10000 , 15000,df))\n",
        "p4 = multiprocessing.Process(target = download, args = (15000 , 20000,df))\n",
        "p5 = multiprocessing.Process(target = download, args = (20000 , 25000,df))\n",
        "p6 = multiprocessing.Process(target = download, args = (25000 , 30000,df))\n",
        "p7 = multiprocessing.Process(target = download, args = (30000 , 35000,df))\n",
        "p8 = multiprocessing.Process(target = download, args = (35000 , 40000,df))\n",
        "p9 = multiprocessing.Process(target = download, args = (40000 , 45000,df))\n",
        "p10 = multiprocessing.Process(target = download, args = (45000 , len(df),df))\n",
        "\n",
        "p1.start()\n",
        "p2.start()\n",
        "p3.start()\n",
        "p4.start()\n",
        "p5.start()\n",
        "p6.start()\n",
        "p7.start()\n",
        "p8.start()\n",
        "p9.start()\n",
        "p10.start()\n",
        "\n",
        "p1.join()\n",
        "p2.join()\n",
        "p3.join()\n",
        "p4.join()\n",
        "p5.join()\n",
        "p6.join()\n",
        "p7.join()\n",
        "p8.join()\n",
        "p9.join()\n",
        "p10.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXtFbodBXMc5"
      },
      "source": [
        "### 4. Combining the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Gd4OBcWM4J",
        "outputId": "836e6364-1705-4c61-8824-12ad1b704c37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49434"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "csvs = [i for i in os.listdir( ) if '.csv' in i]\n",
        "\n",
        "df_ = pd.DataFrame()\n",
        "\n",
        "for csv in csvs:\n",
        "  df = pd.read_csv(csv)\n",
        "  df_ = pd.concat((df,df_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFtKJIxsWdEI"
      },
      "outputs": [],
      "source": [
        "df_.to_csv('final.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}